{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from searchtweets import load_credentials, gen_request_parameters, collect_results\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(\"./sentimentAnalysis/tweets.csv\", usecols=[0, 5], names=[\"target\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = shuffle(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(dataframe, test_size=0.2)\n",
    "train, val = train_test_split(train, test_size=0.2)\n",
    "\n",
    "def df_to_dataset(dataframe, batch_size):\n",
    "  dataframe = dataframe.copy()\n",
    "  texts = dataframe.pop('text')\n",
    "  labels = dataframe.pop('target')\n",
    "  return tf.data.Dataset.from_tensor_slices((texts, labels)).batch(batch_size)\n",
    "\n",
    "batch_size = 320\n",
    "\n",
    "raw_train_ds = df_to_dataset(train, batch_size)\n",
    "raw_val_ds = df_to_dataset(val, batch_size)\n",
    "raw_test_ds = df_to_dataset(test, batch_size)\n",
    "\n",
    "def normalize_data(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  # todo: filter out usernames\n",
    "  usernames = tf.strings.regex_replace(lowercase, '@\"\\B@\\w+\"', 'user')\n",
    "  return tf.strings.regex_replace(usernames, '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "max_features = 10000\n",
    "sequence_length = 56\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=normalize_data,\n",
    "    max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    "\n",
    "train_text = raw_train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-10e1f132df0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../models/sentiment_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m sentiment_model = tf.keras.Sequential([\n\u001b[1;32m      4\u001b[0m   \u001b[0mvectorize_layer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model = model.load('../models/sentiment_model')\n",
    "\n",
    "sentiment_model = tf.keras.Sequential([\n",
    "  vectorize_layer,\n",
    "  model,\n",
    "  layers.Activation('sigmoid')\n",
    "])\n",
    "\n",
    "examples = [\n",
    "  \"Today is a great day!\",\n",
    "  \"This sentence is rather neutral\",\n",
    "  \"This show is terrible!\"\n",
    "]\n",
    "\n",
    "sentiment_model.predict(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cannot read file ./.twitter_keys.yaml\n",
      "Error parsing YAML file; searching for valid environment variables\n",
      "Your credentials are not configured correctly and  you are missing a required field. Please see the  readme for proper configuration\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/searchtweets/credentials.py\u001b[0m in \u001b[0;36m_parse_credentials\u001b[0;34m(search_creds, api_version)\u001b[0m\n\u001b[1;32m     70\u001b[0m         search_args = {\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0;34m\"bearer_token\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msearch_creds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bearer_token\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0;34m\"endpoint\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msearch_creds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"endpoint\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'bearer_token'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-049b110e8fe6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msearch_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_credentials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./.twitter_keys.yaml\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myaml_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"search_tweets_v2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m query = gen_request_parameters(\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"(ether OR eth OR ethereum OR cryptocurrency) -bot -app -is:retweet is:verified lang:en\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtweet_fields\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"id,created_at,text\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/searchtweets/credentials.py\u001b[0m in \u001b[0;36mload_credentials\u001b[0;34m(filename, yaml_key, env_overwrite)\u001b[0m\n\u001b[1;32m    150\u001b[0m                    \u001b[0;32mif\u001b[0m \u001b[0menv_overwrite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                    else merge_dicts(env_vars, yaml_vars))\n\u001b[0;32m--> 152\u001b[0;31m     \u001b[0mparsed_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parse_credentials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerged_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_vars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/searchtweets/credentials.py\u001b[0m in \u001b[0;36m_parse_credentials\u001b[0;34m(search_creds, api_version)\u001b[0m\n\u001b[1;32m     77\u001b[0m                      \u001b[0;34m\" you are missing a required field. Please see the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                      \" readme for proper configuration\")\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msearch_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "search_args = load_credentials(\"./.twitter_keys.yaml\", yaml_key=\"search_tweets_v2\")\n",
    "\n",
    "query = gen_request_parameters(\n",
    "    \"(ether OR eth OR ethereum OR cryptocurrency) -bot -app -is:retweet is:verified lang:en\",\n",
    "    tweet_fields=\"id,created_at,text\",\n",
    "    results_per_call=100)\n",
    "\n",
    "tweets = collect_results(query, max_tweets=100, result_stream_args=search_args)\n",
    "\n",
    "tweet_texts = []\n",
    "\n",
    "for tweet in tweets:\n",
    "    if 'text' not in tweet:\n",
    "        continue\n",
    "    tweet_text = tweet['text']\n",
    "    tweet_text = re.sub(pattern=\"@(\\w*)|(\\\\n)|(https:\\/\\/t\\.co[\\w\\/]*)\", string=tweet_text, repl=\"\")\n",
    "    tweet_texts.append(tweet_text.lower())\n",
    "\n",
    "preds = sentiment_model.predict(tweet_texts).flatten()\n",
    "\n",
    "for i in range(len(tweet_texts)):\n",
    "    t = tweet_texts[i]\n",
    "    p = preds[i]\n",
    "    print(\"{} - {}\".format(t,p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define moving average function\n",
    "def moving_avg(x, n):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0))\n",
    "    return (cumsum[n:] - cumsum[:-n]) / float(n)\n",
    "\n",
    "avg = moving_avg(preds,10)\n",
    "\n",
    "plt.plot(avg, label=\"moving average\")\n",
    "plt.plot(preds.flatten(), \"o\", label=\"predictions from sentiment analysis\")\n",
    "\n",
    "plt.title('Predictions and MA')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
